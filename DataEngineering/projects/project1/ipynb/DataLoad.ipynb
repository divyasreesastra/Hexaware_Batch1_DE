{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "acabb012-e98c-4204-aaac-e554af74e54b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ML Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "138241e0-0062-42af-945c-3f960c0b2a50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import Delta Lake dependencies\n",
    "from delta.tables import *\n",
    "\n",
    "# Read Delta table into a DataFrame\n",
    "df = spark.read.format(\"delta\").load(\"dbfs:/user/hive/warehouse/delta-table\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df071d13-05f2-4ebc-babe-bced30595374",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Model development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4306a3e4-baef-4134-a94e-59f045a60768",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"PropertyTypePrediction\").getOrCreate()\n",
    "\n",
    "# Load data (assuming df_cleaned is already loaded as shown in your image)\n",
    "\n",
    "# 2. Encode Target Variable (Property Type)\n",
    "indexer = StringIndexer(inputCol=\"Property_Type\", outputCol=\"PropertyTypeIndex\")\n",
    "\n",
    "# 3. Feature Selection and Vectorization\n",
    "feature_cols = ['List_Year', 'Assessed_Value', 'Sale_Amount', 'Sales_Ratio']  # Add more numeric features\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "# 4. Split Data into Train and Test Sets\n",
    "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# 5. Train Random Forest Classifier\n",
    "rf = RandomForestClassifier(labelCol=\"PropertyTypeIndex\", featuresCol=\"features\", numTrees=50)\n",
    "\n",
    "# 6. Create a Pipeline\n",
    "pipeline = Pipeline(stages=[indexer, assembler, rf])\n",
    "\n",
    "# 7. Train the Model\n",
    "model = pipeline.fit(train_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6227a1f-38b0-4262-af11-053ffe2ca555",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c647cf34-725d-49dd-9553-eac76f0219e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.6882716049382716\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 8. Make Predictions\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# 9. Evaluate Model Performance\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"PropertyTypeIndex\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy}\")\n",
    "\n",
    "# # Optional: Show predictions\n",
    "# predictions.select(\"Property Type\", \"prediction\", \"features\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dfe15437-a526-41e5-9db4-ad9d92179ac6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5aa7c9b1-dbbd-4cad-930d-164867f7b97a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully at: /Workspace/Users/azuser2373_mml.local@techademy.com/property_type_model\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Generate absolute path\n",
    "save_path = os.path.abspath(\"property_type_model\")\n",
    "\n",
    "# Save the trained model with overwrite mode\n",
    "model.write().overwrite().save(save_path)\n",
    "print(f\"Model saved successfully at: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5a98eb1-f974-42d3-887c-d4da9dae033b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Prediction Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f04a37e-734b-4585-9022-4ad57b9ed200",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n+-------------+----------+--------------------+\n|Property_Type|prediction|            features|\n+-------------+----------+--------------------+\n|Single Family|       0.0|[2013.0,170890.0,...|\n|Single Family|       0.0|[2017.0,35840.0,6...|\n|Single Family|       1.0|[2017.0,46725.0,9...|\n|Single Family|       1.0|[2017.0,81970.0,9...|\n|        Condo|       0.0|[2017.0,101170.0,...|\n|Single Family|       0.0|[2017.0,117150.0,...|\n|  Four Family|       0.0|[2017.0,161490.0,...|\n|Single Family|       0.0|[2017.0,116935.0,...|\n|Single Family|       0.0|[2017.0,52360.0,4...|\n|        Condo|       0.0|[2017.0,17220.0,2...|\n|Single Family|       0.0|[2017.0,23870.0,3...|\n|   Two Family|       1.0|[2017.0,64610.0,1...|\n|        Condo|       0.0|[2017.0,75990.0,7...|\n| Three Family|       1.0|[2017.0,45570.0,1...|\n|Single Family|       0.0|[2018.0,161840.0,...|\n|Single Family|       0.0|[2018.0,14000.0,3...|\n|Single Family|       0.0|[2019.0,96630.0,2...|\n|Single Family|       0.0|[2019.0,42140.0,1...|\n|  Residential|       2.0|[2020.0,126380.0,...|\n|  Residential|       2.0|[2020.0,123370.0,...|\n+-------------+----------+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "# Generate absolute path\n",
    "load_path = os.path.abspath(\"property_type_model\")\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = PipelineModel.load(load_path)\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Use the loaded model to make predictions\n",
    "new_predictions = loaded_model.transform(test_data)\n",
    "new_predictions.select(\"Property_Type\", \"prediction\", \"features\").show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DataLoad",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}