{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39cdaa32-8bff-4fab-9a81-6a7ab9cbb383",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+------+\n|category|       DOB|  name|salary|\n+--------+----------+------+------+\n|       M|1991-04-01|   Ram|  3000|\n|       M|2000-05-19|  Mike|  4000|\n|       M|1978-09-05|Rohini|  4000|\n|       F|1967-12-01| Maria|  4000|\n|       F|1980-02-17| Jenis|  1200|\n+--------+----------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries using select exp\n",
    "from pyspark.sql import SparkSession\n",
    " \n",
    "# Create a spark session\n",
    "spark = SparkSession.builder.appName('pyspark - example join').getOrCreate()\n",
    " \n",
    "# Create data in dataframe\n",
    "data = [(('Ram'), '1991-04-01', 'M', 3000),\n",
    "        (('Mike'), '2000-05-19', 'M', 4000),\n",
    "        (('Rohini'), '1978-09-05', 'M', 4000),\n",
    "        (('Maria'), '1967-12-01', 'F', 4000),\n",
    "        (('Jenis'), '1980-02-17', 'F', 1200)]\n",
    " \n",
    "# Column names in dataframe\n",
    "columns = [\"Name\", \"DOB\", \"Gender\", \"salary\"]\n",
    " \n",
    "# Create the spark dataframe\n",
    "df = spark.createDataFrame(data=data,\n",
    "                           schema=columns)\n",
    "\n",
    "\n",
    "data = df.selectExpr(\"Gender as category\",\"DOB\",\"Name as name\",\"salary\")\n",
    " \n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d49f5b61-1673-43ac-b5c4-dbb85e596c2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+------+------+\n|  Name|date of birth|Gender|salary|\n+------+-------------+------+------+\n|   Ram|   1991-04-01|     M|  3000|\n|  Mike|   2000-05-19|     M|  4000|\n|Rohini|   1978-09-05|     M|  4000|\n| Maria|   1967-12-01|     F|  4000|\n| Jenis|   1980-02-17|     F|  1200|\n+------+-------------+------+------+\n\n+----------+-------------+------+------+\n|personname|date of birth|Gender|salary|\n+----------+-------------+------+------+\n|       Ram|   1991-04-01|     M|  3000|\n|      Mike|   2000-05-19|     M|  4000|\n|    Rohini|   1978-09-05|     M|  4000|\n|     Maria|   1967-12-01|     F|  4000|\n|     Jenis|   1980-02-17|     F|  1200|\n+----------+-------------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    " \n",
    "# Create a spark session\n",
    "spark = SparkSession.builder.appName('pyspark - example join').getOrCreate()\n",
    " \n",
    "# Create data in dataframe\n",
    "data = [(('Ram'), '1991-04-01', 'M', 3000),\n",
    "        (('Mike'), '2000-05-19', 'M', 4000),\n",
    "        (('Rohini'), '1978-09-05', 'M', 4000),\n",
    "        (('Maria'), '1967-12-01', 'F', 4000),\n",
    "        (('Jenis'), '1980-02-17', 'F', 1200)]\n",
    " \n",
    "# Column names in dataframe\n",
    "columns = [\"Name\", \"DOB\", \"Gender\", \"salary\"]\n",
    " \n",
    "# Create the spark dataframe\n",
    "df = spark.createDataFrame(data=data,\n",
    "                           schema=columns)\n",
    "df.withColumnRenamed(\"DOB\",\"date of birth\").show()\n",
    "df.withColumnRenamed(\"DOB\",\"date of birth\").withColumnRenamed(\"Name\",\"personname\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2cae29b-4593-41f7-9cb1-d7b0e8b06718",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------+------+\n|  Name|       DOB|Gender|Amount|\n+------+----------+------+------+\n|   Ram|1991-04-01|     M|  3000|\n|  Mike|2000-05-19|     M|  4000|\n|Rohini|1978-09-05|     M|  4000|\n| Maria|1967-12-01|     F|  4000|\n| Jenis|1980-02-17|     F|  1200|\n+------+----------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    " \n",
    "# Select the 'salary' as 'Amount' using aliasing\n",
    "# Select remaining with their original name\n",
    "data = df.select(col(\"Name\"),col(\"DOB\"),\n",
    "                 col(\"Gender\"),\n",
    "                 col(\"salary\").alias('Amount'))\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a5e56d7-f483-48c0-8d2b-d95560e3e2a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Select_Rename_columns day3",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
